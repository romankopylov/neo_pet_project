{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd51046-7833-4fc6-bb5b-033a31bceb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905ea3a-633e-464b-9464-48bc564f58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e525ac-7d06-491a-b3fe-f7d3470b792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем из файла json путь до дирекотрии с дельтами, наименование таблицы и список полей, являющихся ключами \n",
    "with open('./homework/params.json') as f:\n",
    "    # Загружаем данные из файла\n",
    "    data = json.load(f)\n",
    "    \n",
    "path = data['path']\n",
    "tab_name = data['tab_name']\n",
    "keys_list = data['keys_list']\n",
    "location_delta = data['delta_path']\n",
    "location_mirror = data['mirror_path']\n",
    "\n",
    "print(path, tab_name, keys_list, location_delta, location_mirror, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e0679-794c-442c-8667-424f526b3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание SparkSession с подключением модуля Data Lake\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"task_6\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33693c66-78c1-4ab0-a130-7938d3e1c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путем анализа каталогов с директории с дельтами генерируем список дельт\n",
    "deltas_list = [fname for fname in os.listdir(path) if os.path.isdir(os.path.join(path, fname))]\n",
    "deltas_list.sort()\n",
    "print(deltas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4afad9-b394-4049-b260-9d7224acacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Работа с лог-файлом: \n",
    "# Проверяем, существует ли лог-файл\n",
    "if os.path.exists('./homework/log.csv'):\n",
    "    # Загружаем данные из файла\n",
    "    loaded_df = spark.read.csv('./homework/log.csv', header=True, inferSchema=True, sep=\";\")\n",
    "    # Фильтруем данные по наименованию таблицы, с которой в данные момент работаем \n",
    "    loaded_df = loaded_df.filter(loaded_df['TABLE'] == tab_name)\n",
    "    # Получаем список ID дельт таблицы, загруженных ранее\n",
    "    loadedId_list = loaded_df.select('ID').rdd.flatMap(lambda x: x).collect()\n",
    "    print('Loaded:', loadedId_list)\n",
    "\n",
    "    # Удаляем из списка дельт deltas_list загруженные ранее дельты, которые определили на основании лога\n",
    "    for value in loadedId_list:\n",
    "        if str(value) in deltas_list:\n",
    "            deltas_list.remove(str(value))\n",
    "    \n",
    "    print('To load:', deltas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4fac4-9677-4dba-901d-a389a16872cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# На основании списка ключей таблицы генерируем строку с условиями формата \"oldData.key1 = newData.key1 AND oldData.key2 = newData.key2...\"\n",
    "condition = ' and '.join(['oldData.{} = newData.{}'.format(x, x) for x in keys_list])\n",
    "print(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d248d03-7ef8-4664-8a5c-f4defe914e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем пустой датафрейм для записи логов\n",
    "log_df = pd.DataFrame(columns=['BEGIN', 'END', 'ID', 'TABLE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592f752-84c4-4f90-a3df-b444dec7530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для каждой дельты и списка дельт выполняем действия:\n",
    "for delta in deltas_list:\n",
    "    # Временная метка начала загрузки дельты\n",
    "    time_begin = datetime.datetime.now()\n",
    "\n",
    "    # Загружаем дельту в датафрейм из файла csv\n",
    "    df = spark.read.csv(f'{path}/{delta}/{tab_name}.csv', header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "    # Создаем объект DeltaTable, если он еще не существует\n",
    "    delta_table = DeltaTable.createIfNotExists(spark) \\\n",
    "        .tableName(f\"delta_{tab_name}\") \\\n",
    "        .addColumns(df.schema) \\\n",
    "        .location(f\"{location_delta}{tab_name}\") \\\n",
    "        .execute()\n",
    "\n",
    "    # Выполняем merge с датафреймом, в который загружена дельта, получем промежуточное стостояние зеркала\n",
    "    delta_table.alias(\"oldData\") \\\n",
    "        .merge(df.alias(\"newData\"), condition) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "\n",
    "    # Временная метка окончания загрузки дельты\n",
    "    time_end = datetime.datetime.now()\n",
    "    # Добавляем строку в таблицу логов с данными о времени, дельте и таблице\n",
    "    log_df.loc[len(log_df)] = {'BEGIN': time_begin, 'END': time_end, 'ID': delta, 'TABLE': tab_name}\n",
    "    delta_table.toDF().show()\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b9ded2-c999-4d18-ac75-1decec09f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выгружаем окончательную версию зеркала в фатафрейм и записываем в файл csv\n",
    "mirr_df = delta_table.toDF()\n",
    "mirr_df.write.mode(\"overwrite\").csv(f\"{location_mirror}mirr_{tab_name}.csv\", header=True, sep=\";\")\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b0f25-5231-4945-ba49-1c08bde470ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем данные логов из датафрейма в файл csv\n",
    "# Проверка существования файла\n",
    "if not os.path.exists('./homework/log.csv'):\n",
    "    # Создание файла и запись заголовков и данных\n",
    "    log_df.to_csv('./homework/log.csv', index=False, sep=\";\")\n",
    "else:\n",
    "    # Добавляем строки в конец файла\n",
    "    log_df.to_csv('./homework/log.csv', mode='a', header=False, index=False, sep=\";\")\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25107087-924a-4328-a6fc-6fc596500b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
